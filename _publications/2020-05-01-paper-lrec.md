---
title: "Image Position Prediction in Multimodal Documents"
collection: publications
permalink: /publication/2020-05-01-paper-lrec.md
excerpt: ''
date: 2020-05-01
venue: 'LREC 2020'
paperurl: 'https://www.aclweb.org/anthology/2020.lrec-1.526/'
authors: 'Masayasu Muraoka, Ryosuke Kohita, Etsuko Ishii'
paper: 'https://www.aclweb.org/anthology/2020.lrec-1.526.pdf'
code: 
citation: 
---
Conventional multimodal tasks, such as caption generation and visual question answering, have allowed machines to understand an image by describing or being asked about it in natural language, often via a sentence. Datasets for these tasks contain a large number of pairs of an image and the corresponding sentence as an instance. However, a real multimodal document such as a news article or Wikipedia page consists of multiple sentences with multiple images. Such documents require an advanced skill of jointly considering the multiple texts and multiple images, beyond a single sentence and image, for the interpretation. Therefore, aiming at building a system that can understand multimodal documents, we propose a task called image position prediction (IPP). In this task, a system learns plausible positions of images in a given document. To study this task, we automatically constructed a dataset of 66K multimodal documents with 320K images from Wikipedia articles. We conducted a preliminary experiment to evaluate the performance of a current multimodal system on our task. The experimental results show that the system outperformed simple baselines while the performance is still far from human performance, which thus poses new challenges in multimodal research.